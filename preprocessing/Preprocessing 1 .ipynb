{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a412b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349947e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Absenteeism_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Make sure to create the folder where you would use (ipynb , csv files ... etc) before using the notebook !\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Any edits made to the csv file will reflect on restarting the kernel \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m raw_csv_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mAbsenteeism_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Absenteeism_data.csv'"
     ]
    }
   ],
   "source": [
    "# Make sure to create the folder where you would use (ipynb , csv files ... etc) before using the notebook !\n",
    "# Any edits made to the csv file will reflect on restarting the kernel \n",
    "raw_csv_data = pd.read_csv(\"Absenteeism_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of the original csv file (Data at a glance)\n",
    "#df : data frame \n",
    "df = raw_csv_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to display all the rows and columns \n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(raw_csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good indicator for a python programmar to ensure complete dataset with no missing entries \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37d8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to see the values of all columns \n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Reason for Absence\" code does not have a numerical value (It's a categorical nominal) , we will split it into columns\n",
    "# Quantitative Analysis is giving those categorical nominal data , a numerical meaning \n",
    "# One of the ways to do so , is creating dummy variables : self explanatory binary value that equals 1 if the categorical effect is present and 0 otherwise\n",
    "\n",
    "\n",
    "# Manipulation step 1 : Split Reasons for Absence into dummy variables outside the df\n",
    "# Manipulation step 2 : Group the splitted dummy variables into categories\n",
    "# Manipulation Step 3 : Merge with df after removing the original column\n",
    "\n",
    "reason_columns = pd.get_dummies(df['Reason for Absence'], drop_first = True)\n",
    "# Now we're going to drop reason 0 to avoid multicollinearity (when one variable can be predicted from the others with a high degree of accuracy)\n",
    "# Multicollinearity is a statistical concept where several independent variables in a model are correlated with each other\n",
    "reason_columns.head()\n",
    "# After removing the zero column , the check sum is no longer = 700 and is no longer unique now !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd203f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can consider this (before removing column zero , as a check for logic data ~ No absence for more than 1 reason)\n",
    "# by doing <dataframe_name>['new_column'] it is added to the end , but this method intializes the new column while adding it\n",
    "reason_columns['check'] = reason_columns.sum(axis=1)\n",
    "reason_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccae422",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns['check'].sum(axis=0)\n",
    "# for each reason it counts its occurence \n",
    "# reason_columns.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns['check'].unique()\n",
    "# expected to be 1 , 0 since there's no other values such as missing or multiple reasons\n",
    "# if 0 appears , missing data \n",
    "# if >1 appears , duplicate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns = reason_columns.drop(['check'], axis = 1)\n",
    "reason_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns.loc[:,15:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 27 dummy variables , we should consider grouping them into categories\n",
    "# Grouping the dummy variables into 4 categories = Classing them (classification)\n",
    "# Classification : re-organizing variables into groups in a regression analysis\n",
    "# Reason 1 - 14 : Various Diseases\n",
    "# Reason 15 - 17 : Pregnancy\n",
    "# Reason 18 - 21 : Poisoning\n",
    "# Reason 22 - 28 : Light Diseases\n",
    "\n",
    "# the obtained object is called panda series and not data frame (As well as every other column in the data frame)\n",
    "reason_type_1 = reason_columns.loc[:,1:14].max(axis = 1)\n",
    "reason_type_2 = reason_columns.loc[:,15:17].max(axis = 1)\n",
    "reason_type_3 = reason_columns.loc[:,18:21].max(axis = 1)\n",
    "reason_type_4 = reason_columns.loc[:,22:].max(axis = 1)\n",
    "\n",
    "reason_columns.loc[:,15:17].max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fe6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_type_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Reason for Absence , ID columns from the dataset\n",
    "# axis 0 stands for the y-axis , while axis 1 stands for the x axis \n",
    "df.drop(['ID'], axis = 1) \n",
    "# drop function shows the data frame after removing the column , it is a temporary output and it doesn't yet reflect the frame\n",
    "# Use these lines for permenantly deleting the ID column\n",
    "df = df.drop(['ID'], axis = 1)\n",
    "\n",
    "df = df.drop(['Reason for Absence'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,reason_type_1,reason_type_2,reason_type_3,reason_type_4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ae28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now as we look to the concatenated data frame , [0,1,2,3] seems strange , we need to rename them \n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n",
    "       'Daily Work Load Average', 'Body Mass Index', 'Education',\n",
    "       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason 1', 'Reason 2', 'Reason 3', 'Reason 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = new_column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f23383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns \n",
    "ordered_column_names = ['Reason 1', 'Reason 2', 'Reason 3', 'Reason 4' ,'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n",
    "       'Daily Work Load Average', 'Body Mass Index', 'Education',\n",
    "       'Children', 'Pets', 'Absenteeism Time in Hours']\n",
    "# this is not valid , it only renames but does not order \n",
    "## df.columns = ordered_column_names\n",
    "# instead , you have to do so \n",
    "df = df[ordered_column_names]\n",
    "df.head()\n",
    "# By doing this only , Wrong data are put with wrong labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two ways of extracting unique elements in a column\n",
    "df['Body Mass Index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df['Body Mass Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Body Mass Index'].min())\n",
    "print(df['Body Mass Index'].max())\n",
    "print(len(pd.unique(df['Body Mass Index'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['Body Mass Index'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a checkpoint by creating a copy for the current state of the df \n",
    "df_reason_mod = df.copy()  # version of reasons reordering \n",
    "df_reason_mod[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format='dd/mm/YYYY')\n",
    "type(df_reason_mod['Date'][0]) # => timestamp\n",
    "type(df_reason_mod['Date']) # => series\n",
    "print (df_reason_mod['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e60b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'])\n",
    "# print(type(df_reason_mod['Date'][0]))\n",
    "# df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format ='%d-%m-%Y')\n",
    "# print(type(df_reason_mod['Date']))\n",
    "# print(df_reason_mod['Date'].head(20))\n",
    "# print(type(df_reason_mod['Date'].info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format=' %d / %m / %Y ')\n",
    "(df_reason_mod['Date'][0])\n",
    "(df_reason_mod['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df_reason_mod['Date'][0], dayfirst = True , yearfirst = False, format = \"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod['Date'][0].month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e335ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_months = []\n",
    "for i in range(df_reason_mod.shape[0]):\n",
    "    list_months.append(df_reason_mod['Date'][i].month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eaa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_months\n",
    "df_reason_mod['Month Value'] = list_months\n",
    "df_reason_mod.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71093cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monday : 0 ---> Sunday:6\n",
    "df_reason_mod['Date'][699].weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_weekday(ts):\n",
    "    return ts.weekday() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484db47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod['Day of the Week'] = df_reason_mod['Date'].apply(date_to_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod = df_reason_mod.copy()\n",
    "df_date_mod.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_date_mod['Transportation Expense'][0]))\n",
    "print(type(df_date_mod['Distance to Work'][0]))\n",
    "print(type(df_date_mod['Daily Work Load Average'][0]))\n",
    "print(type(df_date_mod['Age'][0]))\n",
    "print(type(df_date_mod['Body Mass Index'][0]))\n",
    "print(type(df_date_mod['Education'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_date_mod['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy variables using map (0 for highschool , 1 for others)\n",
    "# mapping {highschool->0 key:value}\n",
    "df_date_mod['Education'] = df_date_mod['Education'].map({1:0,2:1,3:1,4:1})\n",
    "# if number of keys != number of values that will result in either naN or error \n",
    "# 1,2,3,4 are keys and [0,1] are values to these keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c729d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod = df_date_mod.drop(['Date'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = df_date_mod.copy()\n",
    "df_preprocessed.head(15)\n",
    "df_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58870b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1945f4d",
   "metadata": {},
   "source": [
    "# Recap \n",
    "## Preprocessing stage : In a nutshell , It's the stage where you need to get your data complete , correlated (later on using regression) and has a numerical meaning for the python libraries to work on\n",
    "### you can use .info() or .describe() to get insights for the data infront of you \n",
    "####   - .info() : displays the count of records + type of each record \n",
    "####   - .describe() : returns statistical info (count , mean , std,min,max,quartiles)\n",
    "### after checking on missing entries , We have three categories of data \n",
    "#### i) useless and deluding data : such as IDs , we can omit them \n",
    "#### ii) categorical data : we can convert them into meaningful numbers using quantitative analysis either by \n",
    "#####      1- Creating predefined (automated) dummy variables by using pandas get_dummies (splits itself into external dataframe derived from chosen one)\n",
    "#####      2- Creating user-defined dummy variables by using the map function (changes the column in its place)\n",
    "#### iii) Dates : we make sure to convert them into Timestamps with defined format on our choice \n",
    "\n",
    "# Important Notes \n",
    "## 1- Make sure to be working on a copy from the original file \n",
    "## 2- Make sure to be saving checkpoints (so you only run 1 needed cell) \n",
    "## 3- No reflection occurs to the files unless the kernel is restarted \n",
    "## 4- No reflection occurs from the drop function if no assignment statement is used (it gives only a temporary output for the expected shape after drop)\n",
    "## 5- useful keywords to look for in this notebook (loc,head,value_counts,map,weekday,apply,.to_datetime,concat,shape)\n",
    "## 6- Renaming columns method is different to reordering them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f32a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
