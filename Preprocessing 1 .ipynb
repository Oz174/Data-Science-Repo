{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349947e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to create the folder where you would use (ipynb , csv files ... etc) before using the notebook !\n",
    "# Any edits made to the csv file will reflect on restarting the kernel \n",
    "raw_csv_data = pd.read_csv(\"Absenteeism_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of the original csv file (Data at a glance)\n",
    "#df : data frame \n",
    "df = raw_csv_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to display all the rows and columns \n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(raw_csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good indicator for a python programmar to ensure complete dataset with no missing entries \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37d8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to see the values of all columns \n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Reason for Absence\" code does not have a numerical value (It's a categorical nominal) , we will split it into columns\n",
    "# Quantitative Analysis is giving those categorical nominal data , a numerical meaning \n",
    "# One of the ways to do so , is creating dummy variables : self explanatory binary value that equals 1 if the categorical effect is present and 0 otherwise\n",
    "\n",
    "\n",
    "# Manipulation step 1 : Split Reasons for Absence into dummy variables outside the df\n",
    "# Manipulation step 2 : Group the splitted dummy variables into categories\n",
    "# Manipulation Step 3 : Merge with df after removing the original column\n",
    "\n",
    "reason_columns = pd.get_dummies(df['Reason for Absence'], drop_first = True)\n",
    "# Now we're going to drop reason 0 to avoid multicollinearity (when one variable can be predicted from the others with a high degree of accuracy)\n",
    "# Multicollinearity is a statistical concept where several independent variables in a model are correlated with each other\n",
    "reason_columns.head()\n",
    "# After removing the zero column , the check sum is no longer = 700 and is no longer unique now !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd203f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can consider this (before removing column zero , as a check for logic data ~ No absence for more than 1 reason)\n",
    "# by doing <dataframe_name>['new_column'] it is added to the end , but this method intializes the new column while adding it\n",
    "reason_columns['check'] = reason_columns.sum(axis=1)\n",
    "reason_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccae422",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns['check'].sum(axis=0)\n",
    "# for each reason it counts its occurence \n",
    "# reason_columns.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns['check'].unique()\n",
    "# expected to be 1 , 0 since there's no other values such as missing or multiple reasons\n",
    "# if 0 appears , missing data \n",
    "# if >1 appears , duplicate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns = reason_columns.drop(['check'], axis = 1)\n",
    "reason_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_columns.loc[:,15:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 27 dummy variables , we should consider grouping them into categories\n",
    "# Grouping the dummy variables into 4 categories = Classing them (classification)\n",
    "# Classification : re-organizing variables into groups in a regression analysis\n",
    "# Reason 1 - 14 : Various Diseases\n",
    "# Reason 15 - 17 : Pregnancy\n",
    "# Reason 18 - 21 : Poisoning\n",
    "# Reason 22 - 28 : Light Diseases\n",
    "\n",
    "# the obtained object is called panda series and not data frame (As well as every other column in the data frame)\n",
    "reason_type_1 = reason_columns.loc[:,1:14].max(axis = 1)\n",
    "reason_type_2 = reason_columns.loc[:,15:17].max(axis = 1)\n",
    "reason_type_3 = reason_columns.loc[:,18:21].max(axis = 1)\n",
    "reason_type_4 = reason_columns.loc[:,22:].max(axis = 1)\n",
    "\n",
    "reason_columns.loc[:,15:17].max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fe6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_type_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Reason for Absence , ID columns from the dataset\n",
    "# axis 0 stands for the y-axis , while axis 1 stands for the x axis \n",
    "df.drop(['ID'], axis = 1) \n",
    "# drop function shows the data frame after removing the column , it is a temporary output and it doesn't yet reflect the frame\n",
    "# Use these lines for permenantly deleting the ID column\n",
    "df = df.drop(['ID'], axis = 1)\n",
    "\n",
    "df = df.drop(['Reason for Absence'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,reason_type_1,reason_type_2,reason_type_3,reason_type_4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ae28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now as we look to the concatenated data frame , [0,1,2,3] seems strange , we need to rename them \n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n",
    "       'Daily Work Load Average', 'Body Mass Index', 'Education',\n",
    "       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason 1', 'Reason 2', 'Reason 3', 'Reason 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = new_column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f23383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns \n",
    "ordered_column_names = ['Reason 1', 'Reason 2', 'Reason 3', 'Reason 4' ,'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n",
    "       'Daily Work Load Average', 'Body Mass Index', 'Education',\n",
    "       'Children', 'Pets', 'Absenteeism Time in Hours']\n",
    "# this is not valid , it only renames but does not order \n",
    "## df.columns = ordered_column_names\n",
    "# instead , you have to do so \n",
    "df = df[ordered_column_names]\n",
    "df.head()\n",
    "# By doing this only , Wrong data are put with wrong labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two ways of extracting unique elements in a column\n",
    "df['Body Mass Index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df['Body Mass Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Body Mass Index'].min())\n",
    "print(df['Body Mass Index'].max())\n",
    "print(len(pd.unique(df['Body Mass Index'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['Body Mass Index'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a checkpoint by creating a copy for the current state of the df \n",
    "df_reason_mod = df.copy()  # version of reasons reordering \n",
    "df_reason_mod[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE TIME FIXED HERE BY USING THE FOLLOWING LINE \n",
    "# df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format = '%d/%m/%Y') gives an error\n",
    "df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format = None, dayfirst=True)\n",
    "type(df_reason_mod['Date'][0]) # => timestamp\n",
    "type(df_reason_mod['Date']) # => series\n",
    "print (df_reason_mod['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod['Date'][5].month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e335ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_months = []\n",
    "for i in range(df_reason_mod.shape[0]):\n",
    "    list_months.append(df_reason_mod['Date'][i].month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eaa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_months\n",
    "df_reason_mod['Month Value'] = list_months\n",
    "df_reason_mod.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71093cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monday : 0 ---> Sunday:6\n",
    "df_reason_mod['Date'][699].weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_weekday(ts):\n",
    "    return ts.weekday() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484db47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_mod['Day of the Week'] = df_reason_mod['Date'].apply(date_to_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod = df_reason_mod.copy()\n",
    "df_date_mod.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_date_mod['Transportation Expense'][0]))\n",
    "print(type(df_date_mod['Distance to Work'][0]))\n",
    "print(type(df_date_mod['Daily Work Load Average'][0]))\n",
    "print(type(df_date_mod['Age'][0]))\n",
    "print(type(df_date_mod['Body Mass Index'][0]))\n",
    "print(type(df_date_mod['Education'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_date_mod['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy variables using map (0 for highschool , 1 for others)\n",
    "# mapping {highschool->0 key:value}\n",
    "df_date_mod['Education'] = df_date_mod['Education'].map({1:0,2:1,3:1,4:1})\n",
    "# if number of keys != number of values that will result in either naN or error \n",
    "# 1,2,3,4 are keys and [0,1] are values to these keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c729d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_mod = df_date_mod.drop(['Date'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = df_date_mod.copy()\n",
    "df_preprocessed.head(15)\n",
    "df_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58870b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1945f4d",
   "metadata": {},
   "source": [
    "# Recap \n",
    "## Preprocessing stage : In a nutshell , It's the stage where you need to get your data complete , correlated (later on using regression) and has a numerical meaning for the python libraries to work on\n",
    "### you can use .info() or .describe() to get insights for the data infront of you \n",
    "####   - .info() : displays the count of records + type of each record \n",
    "####   - .describe() : returns statistical info (count , mean , std,min,max,quartiles)\n",
    "### after checking on missing entries , We have three categories of data \n",
    "#### i) useless and deluding data : such as IDs , we can omit them \n",
    "#### ii) categorical data : we can convert them into meaningful numbers using quantitative analysis either by \n",
    "#####      1- Creating predefined (automated) dummy variables by using pandas get_dummies (splits itself into external dataframe derived from chosen one)\n",
    "#####      2- Creating user-defined dummy variables by using the map function (changes the column in its place)\n",
    "#### iii) Dates : we make sure to convert them into Timestamps with defined format on our choice \n",
    "\n",
    "# Important Notes \n",
    "## 1- Make sure to be working on a copy from the original file \n",
    "## 2- Make sure to be saving checkpoints (so you only run 1 needed cell) \n",
    "## 3- No reflection occurs to the files unless the kernel is restarted \n",
    "## 4- No reflection occurs from the drop function if no assignment statement is used (it gives only a temporary output for the expected shape after drop)\n",
    "## 5- useful keywords to look for in this notebook (loc,head,value_counts,map,weekday,apply,.to_datetime,concat,shape)\n",
    "## 6- Renaming columns method is different to reordering them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f32a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
